{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3308cd42",
   "metadata": {},
   "source": [
    "# Model Comparison – Cricket Match Winner Prediction\n",
    "\n",
    "This notebook trains **all 6 classification models** on the Match dataset, evaluates them side-by-side, and saves trained models + metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca17b18",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978eb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Match_dataset.csv')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'\\nTarget distribution:\\n{df[\"Winner\"].value_counts()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea3bbb",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a635d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived features\n",
    "df['Ranking_Diff'] = df['Team_A_Ranking'] - df['Team_B_Ranking']\n",
    "df['Form_Diff'] = df['Team_A_Form'] - df['Team_B_Form']\n",
    "df['Tech_Diff'] = df['Team_A_Tech_Index'] - df['Team_B_Tech_Index']\n",
    "df['H2H_Diff'] = df['HeadToHead_A_Wins'] - df['HeadToHead_B_Wins']\n",
    "df['Team_A_Won_Toss'] = (df['Toss_Winner'] == 'Team_A').astype(int)\n",
    "df['Toss_Bat'] = (df['Toss_Decision'] == 'Bat').astype(int)\n",
    "\n",
    "# Encode categorical features\n",
    "le_pitch = LabelEncoder()\n",
    "df['Pitch_Type_Enc'] = le_pitch.fit_transform(df['Pitch_Type'])\n",
    "le_stage = LabelEncoder()\n",
    "df['Stage_Enc'] = le_stage.fit_transform(df['Stage'])\n",
    "\n",
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "df['Winner_Enc'] = le_target.fit_transform(df['Winner'])\n",
    "\n",
    "# Select features\n",
    "feature_cols = [\n",
    "    'Team_A_Ranking', 'Team_B_Ranking', 'Team_A_Form', 'Team_B_Form',\n",
    "    'HeadToHead_A_Wins', 'HeadToHead_B_Wins', 'Venue_HomeAdvantage_A',\n",
    "    'Venue_HomeAdvantage_B', 'Avg_T20_Score_Venue', 'Team_A_Tech_Index',\n",
    "    'Team_B_Tech_Index', 'Match_Total', 'Ranking_Diff', 'Form_Diff',\n",
    "    'Tech_Diff', 'H2H_Diff', 'Team_A_Won_Toss', 'Toss_Bat',\n",
    "    'Pitch_Type_Enc', 'Stage_Enc'\n",
    "]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Winner_Enc']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set:     {X_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a277a",
   "metadata": {},
   "source": [
    "## 3. Save Preprocessing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af31a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../model_pkls', exist_ok=True)\n",
    "\n",
    "preprocessing = {\n",
    "    'scaler': scaler,\n",
    "    'le_pitch': le_pitch,\n",
    "    'le_stage': le_stage,\n",
    "    'le_target': le_target,\n",
    "    'feature_cols': feature_cols\n",
    "}\n",
    "joblib.dump(preprocessing, '../model_pkls/preprocessing.pkl')\n",
    "print('Preprocessing artifacts saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c4f60",
   "metadata": {},
   "source": [
    "## 4. Define All 6 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "        use_label_encoder=False, eval_metric='logloss', random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc87cf",
   "metadata": {},
   "source": [
    "## 5. Train, Evaluate & Save Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f'Training: {name}')\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy  = accuracy_score(y_test, y_pred)\n",
    "    auc       = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall    = recall_score(y_test, y_pred)\n",
    "    f1        = f1_score(y_test, y_pred)\n",
    "    mcc       = matthews_corrcoef(y_test, y_pred)\n",
    "    cm        = confusion_matrix(y_test, y_pred).tolist()\n",
    "\n",
    "    all_metrics[name] = {\n",
    "        'Accuracy': round(accuracy, 4),\n",
    "        'AUC': round(auc, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'F1': round(f1, 4),\n",
    "        'MCC': round(mcc, 4),\n",
    "        'Confusion_Matrix': cm\n",
    "    }\n",
    "\n",
    "    print(f'  Accuracy:  {accuracy:.4f}')\n",
    "    print(f'  AUC Score: {auc:.4f}')\n",
    "    print(f'  Precision: {precision:.4f}')\n",
    "    print(f'  Recall:    {recall:.4f}')\n",
    "    print(f'  F1 Score:  {f1:.4f}')\n",
    "    print(f'  MCC Score: {mcc:.4f}')\n",
    "    print(f'\\n  Confusion Matrix:\\n  {confusion_matrix(y_test, y_pred)}')\n",
    "    print(f'\\n  Classification Report:\\n{classification_report(y_test, y_pred, target_names=le_target.classes_)}')\n",
    "\n",
    "    # Save model\n",
    "    safe_name = name.lower().replace(' ', '_')\n",
    "    joblib.dump(model, f'../model_pkls/{safe_name}.pkl')\n",
    "    print(f'  Model saved to ../model_pkls/{safe_name}.pkl\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e8d12",
   "metadata": {},
   "source": [
    "## 6. Save All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model_pkls/metrics.json', 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "print('All metrics saved to ../model_pkls/metrics.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6824a5",
   "metadata": {},
   "source": [
    "## 7. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bffdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_rows = []\n",
    "for name, m in all_metrics.items():\n",
    "    comparison_rows.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': m['Accuracy'],\n",
    "        'AUC': m['AUC'],\n",
    "        'Precision': m['Precision'],\n",
    "        'Recall': m['Recall'],\n",
    "        'F1': m['F1'],\n",
    "        'MCC': m['MCC']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows).set_index('Model')\n",
    "comparison_df.style.highlight_max(axis=0, color='#c6efce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3a06e",
   "metadata": {},
   "source": [
    "## 8. Visual Comparison – Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "comparison_df.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison – All Evaluation Metrics')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "plt.xticks(rotation=25, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaba9fa",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrices – All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d57e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "target_names = le_target.classes_\n",
    "cmaps = ['Blues', 'Greens', 'Oranges', 'Purples', 'YlGn', 'Reds']\n",
    "\n",
    "for idx, (name, m) in enumerate(all_metrics.items()):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    cm = np.array(m['Confusion_Matrix'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmaps[idx],\n",
    "                xticklabels=target_names, yticklabels=target_names, ax=ax)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.suptitle('Confusion Matrices – All Models', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d6b66",
   "metadata": {},
   "source": [
    "## 10. ROC Curves – All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc_val = all_metrics[name]['AUC']\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC={auc_val:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Baseline')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves – All Models')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ae67e",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "All 6 models have been trained, evaluated, and saved. The comparison table and visualisations above provide a clear view of each model's strengths and weaknesses on this dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
